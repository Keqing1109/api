{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KeqingW44448/api/blob/main/RSM8421/5_1_Introduction_to_Word_Vectors.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lo6--yCcFeyD"
      },
      "source": [
        "# Word Vectors in Python with gensim\n",
        "\n",
        "This notebook shows an example of how to use word vectors in Python with gensim."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tUNtF-G6FeyJ"
      },
      "source": [
        "## Load Libraries\n",
        "\n",
        "We're using a new library called `gensim`.  It's a great library for modeling text and comes with pre-trained models that you can easily use in other contexts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-01-20T00:07:48.246163Z",
          "start_time": "2019-01-20T00:07:46.748980Z"
        },
        "id": "BhvSJETNFeyN"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "import numpy as np\n",
        "!pip install gensim\n",
        "import gensim\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from collections import defaultdict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mG0SBtX6FeyX"
      },
      "source": [
        "### Cleaning Our Corpus\n",
        "\n",
        "(corpus is another name for the set of documents under consideration)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-01-20T00:07:48.265144Z",
          "start_time": "2019-01-20T00:07:48.249322Z"
        },
        "id": "U78OdGE6Feya"
      },
      "outputs": [],
      "source": [
        "documents = [\n",
        "    \"Perfect is the enemy of good.\",\n",
        "    \"I'm still learning.\",\n",
        "    \"Life is a journey, not a destination.\",\n",
        "    \"Learning is not attained by chance, it must be sought for with ardor and attended to with diligence.\",\n",
        "    \"Yesterday I was clever, so I changed the world. Today I am wise, so I am changing myself.\",\n",
        "    \"Be curious, not judgmental.\",\n",
        "    \"You don't have to be great to start, but you have to start to be great.,\"\n",
        "    \"Be stubborn about your goals and flexible about your methods.\",\n",
        "    \"Nothing will work unless you do.\",\n",
        "    \"Never give up on a dream just because of the time it will take to accomplish it. The time will pass anyway.\",\n",
        "    \"Anyone who stops learning is old, whether at twenty or eighty.\",\n",
        "    \"Tell me and I forget. Teach me and I remember. Involve me and I learn.\",\n",
        "    \"Change is the end result of all true learning.\",\n",
        "    \"Live as if you were to die tomorrow. Learn as if you were to live forever.\",\n",
        "    \"A learning curve is essential to growth.\",\n",
        "]\n",
        "\n",
        "# remove common words and tokenize\n",
        "stop_words = set('for a of the and to in'.split())\n",
        "texts = [[word for word in document.lower().replace(\"'\", \"\").replace(\".\", \"\").split() if word not in stop_words]\n",
        "         for document in documents]\n",
        "\n",
        "## remove words that appear only once\n",
        "frequency = defaultdict(int)\n",
        "for text in texts:\n",
        "    for token in text:\n",
        "        frequency[token] += 1\n",
        "\n",
        "texts = [[token for token in text if frequency[token] > 1]\n",
        "         for text in texts]\n",
        "texts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AzV1F5nqFeyj"
      },
      "source": [
        "### Creating our Word2Vec Model\n",
        "\n",
        "`gensim` makes it easy to train a Word2Vec model.  All training requires is passing in the corpus."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-01-20T00:07:48.314818Z",
          "start_time": "2019-01-20T00:07:48.268115Z"
        },
        "id": "Wk-yATt6Feyl"
      },
      "outputs": [],
      "source": [
        "model = gensim.models.Word2Vec(texts, vector_size=10, window=2, min_count=1)\n",
        "model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-01-20T00:07:48.326029Z",
          "start_time": "2019-01-20T00:07:48.318240Z"
        },
        "id": "_-i1FfX8Feys"
      },
      "outputs": [],
      "source": [
        "model.wv['live']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vQSRHPLJFeyz"
      },
      "source": [
        "And we can find the most similar words too.  Obviously, our dataset is too small and we won't find anything too interesting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-01-20T00:07:48.341900Z",
          "start_time": "2019-01-20T00:07:48.329322Z"
        },
        "id": "az8FYssqFey1"
      },
      "outputs": [],
      "source": [
        "model.wv.most_similar('live')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0a_7KuYYFezG"
      },
      "source": [
        "### Loading an existing corpus\n",
        "\n",
        "We can load some existing text and train a model on it. In this case, we're going to use `text8`, which is a small subset of Wikipedia (31MB). See: https://github.com/RaRe-Technologies/gensim-data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-01-20T00:07:48.533077Z",
          "start_time": "2019-01-20T00:07:48.345058Z"
        },
        "id": "1wc2Gi_rFezI"
      },
      "outputs": [],
      "source": [
        "import gensim.downloader as api\n",
        "\n",
        "# This could be slow to download...\n",
        "corpus = api.load(\"text8\")\n",
        "corpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-01-20T00:09:11.533094Z",
          "start_time": "2019-01-20T00:07:48.536503Z"
        },
        "id": "1MJQDoLYFezO"
      },
      "outputs": [],
      "source": [
        "# Using small numbers here, probably want to use a bigger corpus, bigger dimensions, and more iterations.\n",
        "model = gensim.models.Word2Vec(corpus, vector_size=10, window=2, epochs=5, min_count=1)\n",
        "model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sr_QsuclFezU"
      },
      "source": [
        "We can get slightly better results (but we really should be using a much bigger corpus)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-01-20T00:09:11.580898Z",
          "start_time": "2019-01-20T00:09:11.536816Z"
        },
        "id": "aRD9XLxMFezV"
      },
      "outputs": [],
      "source": [
        "model.wv.most_similar(\"queen\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-01-20T00:09:11.593176Z",
          "start_time": "2019-01-20T00:09:11.584425Z"
        },
        "id": "M6hKtEnJFeza"
      },
      "outputs": [],
      "source": [
        "model.wv['queen']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "moUaHD6YFezg"
      },
      "source": [
        "### Loading a pre-trained model\n",
        "\n",
        "We can also use Gensim to automatically download and load a pre-trained model, or load it from disk. Since the pre-trained model has much more data, the vectors encode some semantic meaning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-01-20T00:09:36.003788Z",
          "start_time": "2019-01-20T00:09:11.596968Z"
        },
        "id": "RyXl3oWpFezi"
      },
      "outputs": [],
      "source": [
        "# Glove is another word embedding that uses a slightly different technique than word2vec,\n",
        "# however, it has the same properties and API\n",
        "model = api.load(\"glove-wiki-gigaword-50\")\n",
        "model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-01-20T00:09:36.494141Z",
          "start_time": "2019-01-20T00:09:36.006827Z"
        },
        "id": "YVHcUl7rFezn"
      },
      "outputs": [],
      "source": [
        "model.most_similar('queen')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "inBzV93YFezr"
      },
      "source": [
        "Alternatively, we can load the same model directly from disk (the previous calls cache the files in `~/gensim-data/`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-01-20T00:10:00.578988Z",
          "start_time": "2019-01-20T00:09:36.525757Z"
        },
        "id": "hj3GX9K8Fezt"
      },
      "outputs": [],
      "source": [
        "model = gensim.models.KeyedVectors.load_word2vec_format('~/gensim-data/glove-wiki-gigaword-50/glove-wiki-gigaword-50.gz')\n",
        "model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-01-20T00:10:00.847470Z",
          "start_time": "2019-01-20T00:10:00.582233Z"
        },
        "id": "l56WPbt-Fezz"
      },
      "outputs": [],
      "source": [
        "model.most_similar('queen')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-01-20T00:10:00.859049Z",
          "start_time": "2019-01-20T00:10:00.850204Z"
        },
        "id": "oVsR7xchFez3"
      },
      "outputs": [],
      "source": [
        "model['queen']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CiZbTr2tFez8"
      },
      "source": [
        "### Visualizing Words with T-SNE\n",
        "\n",
        "As we saw in the slides, we can visualize the distance between words using T-SNE."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-01-20T00:10:01.154138Z",
          "start_time": "2019-01-20T00:10:00.865386Z"
        },
        "id": "iRgJC7v6Fez8"
      },
      "outputs": [],
      "source": [
        "from sklearn.manifold import TSNE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-01-20T00:10:01.180821Z",
          "start_time": "2019-01-20T00:10:01.159492Z"
        },
        "id": "m8EUNGogFe0A",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# Gather a listing of random words\n",
        "words = ['queen', 'princess', 'dog', 'king', 'cat', 'obama', 'clinton', 'president', 'math', 'brian']\n",
        "\n",
        "vecs = np.array([model[w] for w in words])\n",
        "vecs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-01-20T00:10:01.508727Z",
          "start_time": "2019-01-20T00:10:01.184298Z"
        },
        "id": "efH8UDWbFe0F"
      },
      "outputs": [],
      "source": [
        "vecs_tsne = TSNE(n_components=2, perplexity=3).fit_transform(vecs)\n",
        "vecs_tsne"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-01-20T00:10:01.801307Z",
          "start_time": "2019-01-20T00:10:01.512004Z"
        },
        "id": "vsczS2VtFe0J"
      },
      "outputs": [],
      "source": [
        "ax = sns.scatterplot(x=vecs_tsne[:, 0], y=vecs_tsne[:, 1], s=100)\n",
        "sns.set(font_scale=1.5)\n",
        "[ax.text(p[0], p[1]+10, word, color='black') for word, p in zip(words, vecs_tsne)]\n",
        "pass"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "b-p__-rgXj9i"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.21"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}