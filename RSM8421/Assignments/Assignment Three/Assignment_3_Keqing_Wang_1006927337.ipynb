{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KeqingW44448/api/blob/main/RSM8421/Assignments/Assignment%20Three/Assignment_3_Keqing_Wang_1006927337.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "796835cf",
      "metadata": {
        "id": "796835cf"
      },
      "source": [
        "## Overview\n",
        "In this assignment, we will build a simple song recommender system inspired by the **Word2Vec** algorithm. The main idea is to treat playlists the same way language processing treats sentences.\n",
        "- In a sentence, the words that occur near one another provide information about their meaning.\n",
        "- In a playlist, the songs that co-occur provide information about how listeners associate them.\n",
        "\n",
        "Word2Vec learns an **embedding** (vector representation) for each word by examining the context in which they appear. Words that often appear in similar contexts end up with similar embeddings. We will apply the same idea to music: train embeddings for songs so that songs appearing in similar playlists end up close to one another in the embedding space.  \n",
        "\n",
        "#### Word2Vec Variants: CBOW vs. Skip-Gram\n",
        "\n",
        "Word2Vec has two main variants:\n",
        "- **CBOW (Continuous Bag of Words):** Predict the target word from its surrounding context words.\n",
        "- **Skip-Gram:** Predict the surrounding context words from a single target word.\n",
        "\n",
        "**Example:** Consider the sentence “the quick brown fox jumps.”\n",
        "- **CBOW:** If the target is “brown”, the model uses the context (“the,” “quick,” “fox,” “jumps”) to predict “brown.”\n",
        "- **Skip-Gram:** If the target is “brown”, the model uses “brown” to predict each context word (“the,” “quick,” “fox,” “jumps”).\n",
        "\n",
        "#### Extending to Playlists\n",
        "Now, imagine a short playlist: “Song A, Song B, Song C” with a window size of 1.\n",
        "- **CBOW:** context (Song A, Song C) $\\rightarrow$ predict Song B.\n",
        "- **Skip-Gram:** target Song B $\\rightarrow$ predict (Song A, Song C).\n",
        "In this assignment, we will use the **Skip-Gram** approach. It naturally produces many training pairs and performs well even when some songs appear infrequently.\n",
        "\n",
        "#### Why This Works\n",
        "Once trained, the embeddings can be used to recommend songs that are “close” to one another in the learned space. This works because co-occurrence reflects human preferences: if listeners often place two songs together in playlists, the model learns to embed them nearby.\n",
        "\n",
        "#### Steps You Will Complete\n",
        "1. **Preprocessing:** Prepare the playlists and map each song to a unique numeric ID.\n",
        "2. **Model Training:** Train a Word2Vec-style model using the Skip-Gram method.\n",
        "3. **Embedding Exploration:** Examine the learned embeddings and identify songs the model considers similar.\n",
        "\n",
        "#### Learning Objectives\n",
        "The aim of this assignment is to:\n",
        "- Understand how ideas from language models (e.g., Word2Vec) can be applied to a different domain (music recommendation).\n",
        "- Reflect on the strengths and limitations of this method."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4fb3c1cd",
      "metadata": {
        "id": "4fb3c1cd"
      },
      "source": [
        "## Importing Libraries and Data"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9569ccc9",
      "metadata": {
        "id": "9569ccc9"
      },
      "source": [
        "Let's begin by installing and importing the required libraries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "00b79755",
      "metadata": {
        "id": "00b79755"
      },
      "outputs": [],
      "source": [
        "# Install required packages (uncomment if running for the first time)\n",
        "# %pip install pandas tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dfac63b9",
      "metadata": {
        "id": "dfac63b9"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import pickle\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.preprocessing.sequence import skipgrams\n",
        "import numpy as np\n",
        "from collections import Counter"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7281f594",
      "metadata": {
        "id": "7281f594"
      },
      "source": [
        "The required data for this assignment is provided in the file `training_data.pkl`.\n",
        "This pickle file contains two components:  \n",
        "- **Playlists:** stored as a list of lists, where each inner list represents a unique playlist and each element is a song ID.  \n",
        "- **Metadata:** stored as a DataFrame, where each row corresponds to a song ID and contains information such as the song title and artist.  \n",
        "\n",
        "In this assignment, playlists will serve as the only supervision signal: songs that appear close together in a playlist are considered related. The dataset was collected by Shuo Chen at Cornell University.  \n",
        "\n",
        "**Example:**  \n",
        "- **Playlists:** `[[0, 2, 3], [1, 4]]`  \n",
        "- **Metadata (DataFrame):**  \n",
        "\n",
        "| id | title                             | artist     |  \n",
        "|----|-----------------------------------|------------|  \n",
        "| 0  | Gucci Time (w/ Swizz Beatz)       | Gucci Mane |  \n",
        "| 1  | Aston Martin Music (w/ Drake …)   | Rick Ross  |  \n",
        "| 2  | Get Back Up (w/ Chris Brown)      | T.I.       |  \n",
        "| 3  | Hot Toddy (w/ Jay-Z & Ester Dean) | Usher      |  \n",
        "| 4  | Whip My Hair                      | Willow     |  \n",
        "\n",
        "Here, the first playlist `[0, 2, 3]` corresponds to songs **Gucci Time $\\rightarrow$ Get Back Up $\\rightarrow$ Hot Toddy**, and the second `[1, 4]` corresponds to  **Aston Martin Music $\\rightarrow$ Whip My Hair**.  \n",
        "\n",
        "Let's now load the playlists and metadata, and inspect their shapes and a few examples to sanity-check the parsing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "762dcea8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        },
        "id": "762dcea8",
        "outputId": "fa16dfd1-2af5-4b25-d0cd-d605f2b8eaca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data loaded successfully!\n",
            "Number of playlists: 10738\n",
            "\n",
            "Example Playlists:\n",
            "Playlist 1: ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '2', '42', '43', '44', '45', '46', '47', '48', '20', '49', '8', '50', '51', '52', '53', '54', '55', '56', '57', '25', '58', '59', '60', '61', '62', '3', '63', '64', '65', '66', '46', '47', '67', '2', '48', '68', '69', '70', '57', '50', '71', '72', '53', '73', '25', '74', '59', '20', '46', '75']\n",
            "Playlist 2: ['78', '79', '80', '3', '62', '81', '14', '82', '48', '83', '84', '17', '85', '86', '87', '88', '74', '89', '90', '91', '4', '73', '62', '92', '17', '53', '59', '93', '94', '51', '50', '27', '95', '48', '96', '97', '98', '99', '100', '57', '101', '102', '25', '103', '3', '104', '105', '106', '107', '47', '108', '109', '110', '111', '112', '113', '25', '63', '62', '114', '115', '84', '116', '117', '118', '119', '120', '121', '122', '123', '50', '70', '71', '124', '17', '85', '14', '82', '48', '125', '47', '46', '72', '53', '25', '73', '4', '126', '59', '74', '20', '43', '127', '128', '129', '13', '82', '48', '130', '131', '132', '133', '134', '135', '136', '137', '59', '46', '138', '43', '20', '139', '140', '73', '57', '70', '141', '3', '1', '74', '142', '143', '144', '145', '48', '13', '25', '146', '50', '147', '126', '59', '20', '148', '149', '150', '151', '152', '56', '153', '154', '155', '156', '157', '158', '159', '160', '161', '162', '163', '164', '165', '166', '167', '168', '169', '170', '171', '172', '173', '174', '175', '60', '176', '51', '177', '178', '179', '180', '181', '182', '183', '184', '185', '57', '186', '187', '188', '189', '190', '191', '46', '192', '193', '194', '195', '196', '197', '198', '25', '199', '200', '49', '201', '100', '202', '203', '204', '205', '206']\n",
            "\n",
            "First 5 songs in playlist 9: ['Speechless', 'Alright', 'On The The Next One (feat. Swizz Beatz)', 'If Love So Nice', \"It's A Pity\"]\n",
            "\n",
            "Song metadata sample:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "                                               title      artist\n",
              "0                       Gucci Time (w\\/ Swizz Beatz)  Gucci Mane\n",
              "1  Aston Martin Music (w\\/ Drake & Chrisette Mich...   Rick Ross\n",
              "2                      Get Back Up (w\\/ Chris Brown)        T.I.\n",
              "3                 Hot Toddy (w\\/ Jay-Z & Ester Dean)       Usher\n",
              "4                                       Whip My Hair      Willow"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-883dfb4a-a286-43d0-9a42-35e9739f50db\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>artist</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Gucci Time (w\\/ Swizz Beatz)</td>\n",
              "      <td>Gucci Mane</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Aston Martin Music (w\\/ Drake &amp; Chrisette Mich...</td>\n",
              "      <td>Rick Ross</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Get Back Up (w\\/ Chris Brown)</td>\n",
              "      <td>T.I.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Hot Toddy (w\\/ Jay-Z &amp; Ester Dean)</td>\n",
              "      <td>Usher</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Whip My Hair</td>\n",
              "      <td>Willow</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-883dfb4a-a286-43d0-9a42-35e9739f50db')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-883dfb4a-a286-43d0-9a42-35e9739f50db button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-883dfb4a-a286-43d0-9a42-35e9739f50db');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-0a80b14d-770e-45f6-b164-d4c68fb65f36\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-0a80b14d-770e-45f6-b164-d4c68fb65f36')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-0a80b14d-770e-45f6-b164-d4c68fb65f36 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"display(songs_df\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"title\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"Aston Martin Music (w\\\\/ Drake & Chrisette Michelle)\",\n          \"Whip My Hair\",\n          \"Get Back Up (w\\\\/ Chris Brown)\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"artist\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"Rick Ross\",\n          \"Willow\",\n          \"T.I.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Load the playlists and metadata using pickle\n",
        "# Make sure the file 'training_data.pkl' is in the same directory as this notebook\n",
        "with open('training_data.pkl', 'rb') as f:\n",
        "    data = pickle.load(f)\n",
        "\n",
        "# Extract playlists (list of lists) and metadata (DataFrame)\n",
        "playlists = data['train_playlists']\n",
        "songs_df = data['songs_info']\n",
        "\n",
        "# Basic dataset info\n",
        "print(\"Data loaded successfully!\")\n",
        "print(f\"Number of playlists: {len(playlists)}\")\n",
        "\n",
        "# Show a couple of example playlists\n",
        "print(\"\\nExample Playlists:\")\n",
        "print(\"Playlist 1:\", playlists[0])\n",
        "print(\"Playlist 2:\", playlists[1])\n",
        "\n",
        "# Preview a playlist with song titles\n",
        "playlist_idx = 8\n",
        "print(f\"\\nFirst 5 songs in playlist {playlist_idx+1}:\",[songs_df.iloc[int(idx)].title for idx in playlists[playlist_idx]][:5])\n",
        "\n",
        "# Show the first few rows of the metadata table\n",
        "print(\"\\nSong metadata sample:\")\n",
        "display(songs_df.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9d78ad3a",
      "metadata": {
        "id": "9d78ad3a"
      },
      "source": [
        "## Part 1. Data Preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "38176f91-793e-4d2b-ba91-2670c1ac0d3e",
      "metadata": {
        "id": "38176f91-793e-4d2b-ba91-2670c1ac0d3e"
      },
      "source": [
        "Now that we have loaded the playlists and metadata and checked that everything looks correct, the next step is to **prepare the data for training**.  \n",
        "\n",
        "An embedding can be thought of as a big lookup table, where each row stores the vector for one item (here, a song). This table has rows numbered from `0` up to `V`, where `V` is the number of unique songs. The first row (`0`) is reserved for a special placeholder called the padding token (`<pad>`). All actual songs start from row `1` onward.\n",
        "\n",
        "**The issue:**  \n",
        "- The raw song IDs in our playlists are arbitrary labels (e.g., 5, 20, 2172).  \n",
        "- They don’t necessarily start at 1 or form a contiguous sequence.  \n",
        "- If we used them directly, we could either run into errors (ID out of range) or map to the wrong row in the lookup table.  \n",
        "\n",
        "**Solution:** build a mapping.  \n",
        "- Collect all unique raw song IDs and assign each a new index in `1, 2, …, V`.  \n",
        "- Store both mappings so we can translate between raw IDs and indices.  \n",
        "- Rewrite each playlist to use these contiguous indices.  \n",
        "\n",
        "**Example:**  \n",
        "Suppose we have two playlists:  \n",
        "- `[5, 20, 2172]`  \n",
        "- `[20, 5]`  \n",
        "\n",
        "The unique IDs are `{5, 20, 2172}`, so the vocabulary size is 3. We map:  \n",
        "- `5` $\\rightarrow$ `1`  \n",
        "- `20` $\\rightarrow$ `2`  \n",
        "- `2172` $\\rightarrow$ `3`  \n",
        "\n",
        "With this mapping, the playlists become:\n",
        "- `[1, 2, 3]`  \n",
        "- `[2, 1]`  \n",
        "\n",
        "Now every playlist uses contiguous indices in `1, 2, …, V`, exactly matching the rows of the lookup table.  \n",
        "\n",
        "**Your task:** Complete the code below to:  \n",
        "1. Build the vocabulary of unique song IDs.  \n",
        "2. Create `raw_to_idx` and `idx_to_raw` mappings that map raw song IDs to indices and vice versa, respectively.  \n",
        "3. Remap playlists into index form."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c4980222",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c4980222",
        "outputId": "f0c9fc56-58b0-4205-bad5-6670f3ce853c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary size: 9148\n",
            "Total song occurrences: 33813\n"
          ]
        }
      ],
      "source": [
        "# Use only a subset of playlists to speed up training\n",
        "num_playlists = 200\n",
        "\n",
        "# Flatten the first `num_playlists` playlists into one long list of song IDs\n",
        "flat = [int(x) for pl in playlists[:num_playlists] for x in pl]\n",
        "\n",
        "# Count how often each raw song ID appears across these playlists. Output looks like: {song_id: count}\n",
        "counts = Counter(flat)\n",
        "\n",
        "# Sorted list of all unique raw song IDs (the vocabulary)\n",
        "vocab_raw_ids = sorted(counts.keys())\n",
        "# Vocabulary size = number of unique songs\n",
        "vocab_size = len(vocab_raw_ids)\n",
        "print(\"Vocabulary size:\", vocab_size)\n",
        "print(\"Total song occurrences:\", len(flat))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ff3b2107",
      "metadata": {
        "id": "ff3b2107"
      },
      "outputs": [],
      "source": [
        "### TODO ###\n",
        "# Create mappings between raw song IDs (vocab_raw_ids) and contiguous indices (1, 2, ..., V)\n",
        "# Hint: use dictionary comprehensions\n",
        "# raw_to_idx: raw ID -> contiguous index, e.g. {5: 1, 20: 2, 2172: 3}\n",
        "raw_to_idx = {raw_id: index for index, raw_id in enumerate(vocab_raw_ids, start=1)}\n",
        "# idx_to_raw: contiguous index -> raw ID\n",
        "idx_to_raw = {index: raw_id for index, raw_id in enumerate(vocab_raw_ids, start=1)}\n",
        "### END OF TODO ###\n",
        "\n",
        "# Remap each playlist from raw IDs into contiguous indices (1, 2, ..., V)\n",
        "# Example: [5, 20, 2172] -> [1, 2, 3]\n",
        "playlists_idx = [[raw_to_idx[int(x)] for x in pl if int(x) in raw_to_idx] for pl in playlists[:num_playlists]]\n",
        "\n",
        "# Build lookup dictionaries for human-readable printing\n",
        "# id_to_song: contiguous index (1, 2, ..., V) -> \"Title by Artist\"\n",
        "id_to_song = {i: songs_df.iloc[idx_to_raw[i]].title + \" by \" + songs_df.iloc[idx_to_raw[i]].artist for i in range(1, vocab_size + 1)}\n",
        "# song_to_id: \"Title by Artist\" -> contiguous index\n",
        "song_to_id = {name: i for i, name in id_to_song.items()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9b2f2c76",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9b2f2c76",
        "outputId": "4f9d0f5e-3b87-47b8-e9c7-502e1a032890"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First 5 entries in id_to_song:\n",
            "\tID 1: Gucci Time (w\\/ Swizz Beatz) by Gucci Mane\n",
            "\tID 2: Aston Martin Music (w\\/ Drake & Chrisette Michelle) by Rick Ross\n",
            "\tID 3: Get Back Up (w\\/ Chris Brown) by T.I.\n",
            "\tID 4: Hot Toddy (w\\/ Jay-Z & Ester Dean) by Usher\n",
            "\tID 5: Whip My Hair by Willow\n",
            "\n",
            "First 5 entries in song_to_id:\n",
            "\tSong Gucci Time (w\\/ Swizz Beatz) by Gucci Mane: 1\n",
            "\tSong Aston Martin Music (w\\/ Drake & Chrisette Michelle) by Rick Ross: 2\n",
            "\tSong Get Back Up (w\\/ Chris Brown) by T.I.: 3\n",
            "\tSong Hot Toddy (w\\/ Jay-Z & Ester Dean) by Usher: 4\n",
            "\tSong Whip My Hair by Willow: 5\n",
            "\n",
            "Top 5 most frequent songs:\n",
            "  Song Who's That Chick by Rihanna -> ID 13 (appears 204 times)\n",
            "  Song Holding You Down (Goin' In Circles) by Jazmine Sullivan -> ID 70 (appears 166 times)\n",
            "  Song Take A Chance by Micah G -> ID 3188 (appears 136 times)\n",
            "  Song Be Without You by Mary J. Blige -> ID 50 (appears 125 times)\n",
            "  Song Ants In Yuh Sugar Pan by Jamesy P -> ID 81 (appears 124 times)\n"
          ]
        }
      ],
      "source": [
        "# Print a few samples to sanity-check the mappings\n",
        "print(\"First 5 entries in id_to_song:\")\n",
        "for i, (id, song) in enumerate(id_to_song.items()):\n",
        "    if i >= 5: break\n",
        "    print(f\"\\tID {id}: {song}\")\n",
        "\n",
        "print(\"\\nFirst 5 entries in song_to_id:\")\n",
        "for i, (song, id) in enumerate(song_to_id.items()):\n",
        "    if i >= 5: break\n",
        "    print(f\"\\tSong {song}: {id}\")\n",
        "\n",
        "print(f\"\\nTop 5 most frequent songs:\")\n",
        "for i, (idx, count) in enumerate(list(counts.most_common(5))):\n",
        "    print(f\"  Song {id_to_song[idx]} -> ID {idx} (appears {count} times)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "544ade6c",
      "metadata": {
        "id": "544ade6c"
      },
      "source": [
        "## Part 2. Create skip-gram training pairs and labels from playlists"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3bc4918b",
      "metadata": {
        "id": "3bc4918b"
      },
      "source": [
        "In the previous step, we prepared the playlists so that every song was mapped to a contiguous index between `1` and `V`. Now we want to turn those playlists into training examples that can teach our embedding model.\n",
        "\n",
        "We first turn each playlist (a sequence of song indices) into many small training examples. For every position in a playlist we treat the song at that position as the **target** and the nearby songs (within a fixed window to the left and right) as its **context**. Each (target, context) pair is a **positive** example because those two songs actually appeared near each other. To train a model, we also create **negative** examples by pairing the same targets with randomly chosen other songs that did not appear next to them. Positives get label `1`, negatives get label `0`.\n",
        "\n",
        "**Example (window size = 1):**  \n",
        "Playlist: `[5, 7, 9]`  \n",
        "\n",
        "- Target `5`, context `[7]` $\\rightarrow$ pair `(5, 7)`  \n",
        "- Target `7`, context `[5, 9]` $\\rightarrow$ pairs `(7, 5)`, `(7, 9)`  \n",
        "- Target `9`, context `[7]` $\\rightarrow$ pair `(9, 7)`  \n",
        "\n",
        "Positive set: `[(5, 7), (7, 5), (7, 9), (9, 7)]` (all labeled 1)  \n",
        "If we add 2 random negatives per positive (e.g. `(5, 12)`, `(5, 3)`, …) those get label 0.\n",
        "\n",
        "This manual process involves:\n",
        "- Sliding a window around each song.\n",
        "- Emitting all (target, context) positives.\n",
        "- Sampling random contexts for negatives.\n",
        "- Returning parallel arrays: `targets`, `contexts`, `labels`."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "78221092",
      "metadata": {
        "id": "78221092"
      },
      "source": [
        "We manually described how to form (target, context) pairs. Keras also provides a helper `tf.keras.preprocessing.sequence.skipgrams` that automates this and (optionally) adds negative samples. Here is what it does when we call it:\n",
        "\n",
        "**Call pattern in our code:**\n",
        "```python\n",
        "pairs, labels = skipgrams(\n",
        "    sequence=seq,\n",
        "    vocabulary_size=vocab_size,\n",
        "    window_size=WINDOW_SIZE,\n",
        "    negative_samples=NEGATIVE_SAMPLES\n",
        ")\n",
        "```\n",
        "\n",
        "**Steps performed internally:**\n",
        "1. Positive window sampling  \n",
        "   - For each position `i` in `sequence`, it picks context indices within `[-window_size, +window_size]` (excluding `i`).  \n",
        "   - Each valid (target, context) becomes a positive pair.\n",
        "\n",
        "2. Negative sampling  \n",
        "   - For every positive pair, it draws up to `negative_samples` random context indices in the range `[1, ..., vocabulary_size)` (ignoring some invalid draws like the target itself). So we need to pass `vocab_size + 1` as the argument to make sure all sampled indices are valid.\n",
        "   - These synthetic (target, random_context) pairs are labeled as negatives.\n",
        "\n",
        "3. Output format  \n",
        "   - `pairs` is a Python list of `[target_index, context_index]`.\n",
        "   - `labels` is a list of the same length where `1` = real (positive) and `0` = negative.\n",
        "\n",
        "**Why we still post‑process:**\n",
        "- We accumulate `pairs` and `labels` from every playlist into large arrays for training.\n",
        "\n",
        "**Key parameters you can tune:**\n",
        "- `window_size`: how far left/right to look for context.\n",
        "- `negative_samples`: how many negatives per positive (higher → larger, more imbalanced dataset).\n",
        "\n",
        "This utility saves you from manually:\n",
        "- Sliding the window,\n",
        "- Building positive lists,\n",
        "- Generating negatives and labeling them.\n",
        "\n",
        "So one line gives you a ready mixed (targets, contexts) + (labels) dataset for that sequence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "41b2fef2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "41b2fef2",
        "outputId": "f2b3daf8-61fc-47a3-a181-1f093ea0fac7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Skip-gram generation complete.\n",
            "Total training pairs: 1792020\n",
            "Positive samples: 298664\n",
            "Negative samples: 1493356\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# -----------------------------\n",
        "# Hyperparameters\n",
        "# -----------------------------\n",
        "WINDOW_SIZE = 10\n",
        "NEGATIVE_SAMPLES = 5\n",
        "MAX_PAIRS_PER_PLAYLIST = 10_000\n",
        "\n",
        "# Custom Skip-Gram Generator\n",
        "\n",
        "def generate_skipgrams(sequence, window_size, vocab_size, negative_samples=5):\n",
        "    pairs = []\n",
        "    labels = []\n",
        "\n",
        "    seq = [int(s) for s in sequence]  # ensure pure python ints\n",
        "\n",
        "    for i, target in enumerate(seq):\n",
        "        start = max(0, i - window_size)\n",
        "        end = min(len(seq), i + window_size + 1)\n",
        "\n",
        "        for j in range(start, end):\n",
        "            if i == j:\n",
        "                continue\n",
        "\n",
        "            context = seq[j]\n",
        "\n",
        "            # Positive pair\n",
        "            pairs.append([target, context])\n",
        "            labels.append(1)\n",
        "\n",
        "            # Negative samples\n",
        "            for _ in range(negative_samples):\n",
        "                negative = np.random.randint(1, vocab_size + 1)\n",
        "                pairs.append([target, negative])\n",
        "                labels.append(0)\n",
        "\n",
        "    return pairs, labels\n",
        "\n",
        "# Build All Training Pairs\n",
        "\n",
        "pairs_all = []\n",
        "labels_all = []\n",
        "\n",
        "rng = np.random.default_rng(42)\n",
        "\n",
        "for seq in playlists_idx:\n",
        "    if len(seq) < 2:\n",
        "        continue\n",
        "\n",
        "    # Generate pairs\n",
        "    pairs, labels = generate_skipgrams(\n",
        "        sequence=seq,\n",
        "        window_size=WINDOW_SIZE,\n",
        "        vocab_size=vocab_size,\n",
        "        negative_samples=NEGATIVE_SAMPLES\n",
        "    )\n",
        "\n",
        "    # Subsample if too large\n",
        "    if len(pairs) > MAX_PAIRS_PER_PLAYLIST:\n",
        "        choose = rng.choice(len(pairs), size=MAX_PAIRS_PER_PLAYLIST, replace=False)\n",
        "        pairs = [pairs[i] for i in choose]\n",
        "        labels = [labels[i] for i in choose]\n",
        "\n",
        "    pairs_all.extend(pairs)\n",
        "    labels_all.extend(labels)\n",
        "\n",
        "\n",
        "# Convert to NumPy arrays\n",
        "\n",
        "pairs_all = np.array(pairs_all, dtype=np.int32)\n",
        "labels_all = np.array(labels_all, dtype=np.float32)\n",
        "\n",
        "targets_np = pairs_all[:, 0]\n",
        "contexts_np = pairs_all[:, 1]\n",
        "\n",
        "print(\"✅ Skip-gram generation complete.\")\n",
        "print(f\"Total training pairs: {len(pairs_all)}\")\n",
        "print(f\"Positive samples: {int(labels_all.sum())}\")\n",
        "print(f\"Negative samples: {len(labels_all) - int(labels_all.sum())}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f089151f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f089151f",
        "outputId": "15a5b3dc-551c-4dd2-bcfc-1e5aaa51f6c1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated: 1792020 training pairs | 298664 positives | 1493356 negatives\n",
            "Example pairs: [[  16 5950]\n",
            " [  46    3]\n",
            " [  54 2477]\n",
            " [  34  945]\n",
            " [  58 8665]]\n",
            "Example labels: [0. 1. 0. 0. 0.]\n",
            "\n",
            "Target shape: (1792020,)\n",
            "Context shape: (1792020,)\n"
          ]
        }
      ],
      "source": [
        "print(f\"Generated: {len(pairs_all)} training pairs | {int(labels_all.sum())} positives | {len(labels_all)-int(labels_all.sum())} negatives\")\n",
        "print(f\"Example pairs: {pairs_all[:5]}\")\n",
        "print(f\"Example labels: {labels_all[:5]}\\n\")\n",
        "\n",
        "print(f\"Target shape: {targets_np.shape}\")\n",
        "print(f\"Context shape: {contexts_np.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a78f168b",
      "metadata": {
        "id": "a78f168b"
      },
      "source": [
        "## Question 1.  \n",
        "When we create negative samples, we are drawing random song IDs without checking whether that pair might actually appear together in a real playlist. This means that sometimes a \"negative\" pair could in fact be a true (target, context) pair. Would that be a problem? If so, how should we fix it? If not, why is that acceptable?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e663e98d",
      "metadata": {
        "id": "e663e98d"
      },
      "source": [
        "## Part 3. Build and compile the Keras Word2Vec model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "52652f2a",
      "metadata": {
        "id": "52652f2a"
      },
      "source": [
        "Now that we have training pairs, we need a model that can learn embeddings for each song.  In Keras, we can use the **`Embedding`** layer to do this. An embedding is just a table:  \n",
        "- Each row is a vector for one item (here, one song).  \n",
        "- Instead of manually creating and updating this table, `layers.Embedding` handles it during training.  \n",
        "\n",
        "The `Embedding` layer in Keras takes two key arguments:  \n",
        "- `input_dim`: the number of unique items we have (the vocabulary size).  \n",
        "- `output_dim`: the length of each embedding vector (the number of features per song).  \n",
        "\n",
        "Example: if `input_dim = 1000` and `output_dim = 32`, the layer will learn a `1000 × 32` matrix,  \n",
        "where each of the 1000 rows is a 32-dimensional embedding for one song.  \n",
        "\n",
        "Once we have embeddings, we need to compare them:  \n",
        "- **`layers.Dot`** computes the dot product between two vectors.  \n",
        "  - If vectors point in a similar direction, the dot product is large.  \n",
        "  - If vectors are very different, the dot product is small or negative.  \n",
        "- **`layers.Activation`** maps this raw score into a probability-like output (e.g., between 0 and 1).  \n",
        "\n",
        "This makes training possible as a **binary classification task**:  \n",
        "- Positive pairs (real co-occurrences) should get outputs close to 1.  \n",
        "- Negative pairs (randomly sampled) should get outputs close to 0.  \n",
        "\n",
        "**Step-by-step flow:**  \n",
        "1. Input two integers: one for the **target song**, one for the **context song**.  \n",
        "2. `Embedding` layer looks up each song's vector.  \n",
        "3. `Dot` computes similarity: target vector $\\cdot$ context vector.  \n",
        "4. `Activation` turns this score into a probability for classification.  \n",
        "\n",
        "**Manual example:**  \n",
        "- Suppose row 3 = `[0.2, -0.5]`, row 8 = `[0.7, 0.1]`.  \n",
        "- Input: target `3`, context `8`.  \n",
        "- Lookup: `[0.2, -0.5]` and `[0.7, 0.1]`.  \n",
        "- Dot product = $0.2 \\times 0.7 + (-0.5) \\times 0.1 = 0.14 - 0.05 = 0.09$.  \n",
        "- Activation transforms `0.09` into a probability-like output (e.g., `0.52`).  \n",
        "\n",
        "**Your task:** complete the code below to:  \n",
        "1. Define the embedding model with `Embedding`, `Dot`, and `Activation` layers.  \n",
        "2. Compile the model with an optimizer and loss function suitable for binary classification."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1ef9f613",
      "metadata": {
        "id": "1ef9f613"
      },
      "outputs": [],
      "source": [
        "# Build the Keras Word2Vec model\n",
        "def create_word2vec_model(vocab_size, embedding_dim=32):\n",
        "    \"\"\"\n",
        "    Build a Skip-Gram with Negative Sampling (SGNS) model.\n",
        "\n",
        "    Args:\n",
        "        vocab_size: number of unique songs (rows in the embedding table) +1 for padding index 0.\n",
        "        embedding_dim: size of each song vector.\n",
        "\n",
        "    Returns:\n",
        "        model: Keras Model that takes (target_idx, context_idx) and outputs p(real_pair).\n",
        "        embedding_layer: the shared Embedding layer whose weights are the learned song vectors.\n",
        "    \"\"\"\n",
        "\n",
        "    # Input layers\n",
        "    target_input = keras.Input(shape=(), name='target')\n",
        "    context_input = keras.Input(shape=(), name='context')\n",
        "\n",
        "    ### TODO ###\n",
        "    # Embedding layer(s) for songs\n",
        "    # Each lookup returns a vector of length `embedding_dim`\n",
        "    embedding_layer = layers.Embedding(\n",
        "        input_dim=vocab_size + 1,\n",
        "        output_dim=embedding_dim,\n",
        "        name='song_embeddings'\n",
        "    )\n",
        "\n",
        "    # Lookup vectors for target and context (shape: (batch, embedding_dim))\n",
        "    target_embedding = embedding_layer(target_input)\n",
        "    context_embedding = embedding_layer(context_input)\n",
        "\n",
        "    # Similarity score: dot product of the two vectors (per example)\n",
        "    dot_product = layers.Dot(axes=1)([target_embedding, context_embedding])\n",
        "\n",
        "    # Probability that (target, context) is a real pair\n",
        "    output = layers.Activation(\"sigmoid\")(dot_product)\n",
        "    ### END OF TODO ###\n",
        "\n",
        "    # Build the model\n",
        "    model = keras.Model(inputs=[target_input, context_input], outputs=output)\n",
        "\n",
        "    return model, embedding_layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "972c48a3",
      "metadata": {
        "id": "972c48a3"
      },
      "outputs": [],
      "source": [
        "### TODO ###\n",
        "# Create the model\n",
        "embedding_dim = 32\n",
        "model, embedding_layer = create_word2vec_model(vocab_size, embedding_dim)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(\n",
        "    optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
        "    loss=\"binary_crossentropy\",\n",
        "    metrics=[\"accuracy\"]\n",
        ")\n",
        "### END OF TODO ###"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c44b1347",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 361
        },
        "id": "c44b1347",
        "outputId": "d6271c35-a341-43ad-92ba-82c871f25817"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
              "│ target (\u001b[38;5;33mInputLayer\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m)            │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ context             │ (\u001b[38;5;45mNone\u001b[0m)            │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ song_embeddings     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)        │    \u001b[38;5;34m292,768\u001b[0m │ target[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],     │\n",
              "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │ context[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dot (\u001b[38;5;33mDot\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │          \u001b[38;5;34m0\u001b[0m │ song_embeddings[\u001b[38;5;34m…\u001b[0m │\n",
              "│                     │                   │            │ song_embeddings[\u001b[38;5;34m…\u001b[0m │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ activation          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │          \u001b[38;5;34m0\u001b[0m │ dot[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         │\n",
              "│ (\u001b[38;5;33mActivation\u001b[0m)        │                   │            │                   │\n",
              "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
              "│ target (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)            │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ context             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)            │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ song_embeddings     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        │    <span style=\"color: #00af00; text-decoration-color: #00af00\">292,768</span> │ target[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],     │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │ context[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dot (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dot</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ song_embeddings[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│                     │                   │            │ song_embeddings[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ activation          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dot[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)        │                   │            │                   │\n",
              "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m292,768\u001b[0m (1.12 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">292,768</span> (1.12 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m292,768\u001b[0m (1.12 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">292,768</span> (1.12 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Print model summary\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dbea4080",
      "metadata": {
        "id": "dbea4080"
      },
      "source": [
        "## Question 2.  \n",
        "In your Keras model did you use a *shared embedding table* for both the target and the context songs (the same layer handles both inputs) or *separate embedding tables*? Is one of these approaches \"wrong\"? What could be the benefits and drawbacks of each choice, and does the answer change depending on whether we use Skip-Gram or CBOW?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "caee52cd",
      "metadata": {
        "id": "caee52cd"
      },
      "source": [
        "## Question 3.\n",
        "What metric do you think would make sense to include in `model.compile()` to track the model’s learning? Do you think that metric would actually reflect the quality of the learned embeddings? Why? Similarly, what about using a `validation_split` during training, does that give us a meaningful way to evaluate this model?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1b60585f",
      "metadata": {
        "id": "1b60585f"
      },
      "source": [
        "## Question 4.\n",
        "Is defining a `Dense` layer after the dot product different from simply passing the dot product directly into an activation function? If so, is this still a valid approach, or does it break the model? If it is valid, what are the concrete benefits and drawbacks?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e863b262",
      "metadata": {
        "id": "e863b262"
      },
      "source": [
        "## Part 4. Train the model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "145232e2",
      "metadata": {
        "id": "145232e2"
      },
      "source": [
        "Now that the model is defined and compiled, the next step is to **train it**. Training means feeding the model batches of `(target, context)` pairs along with their labels:  \n",
        "- Label `1` for real co-occurrences (positive pairs).  \n",
        "- Label `0` for randomly generated negatives.  \n",
        "\n",
        "The model adjusts the values in the embedding table so that:  \n",
        "- Real pairs get higher scores.  \n",
        "- Negative pairs get lower scores.  \n",
        "\n",
        "We use `model.fit()` to run training. Some important arguments are:  \n",
        "- `x`: the inputs (here, the arrays for target songs and context songs).  \n",
        "- `y`: the labels for each pair.  \n",
        "- `batch_size`: how many examples are processed before updating the model.  \n",
        "- `epochs`: how many full passes through the dataset.  \n",
        "- `shuffle`: whether to shuffle data each epoch.  \n",
        "- `validation_split`: reserve a fraction of the data for validation.  \n",
        "\n",
        "**Your Task:** Complete and run the code below to train the model.  \n",
        "Set a reasonable number of epochs so that training runs in a reasonable amount of time but still shows progress."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "891e195a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "891e195a",
        "outputId": "c5377d20-9e57-4638-cf8d-dc545ad81cfb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training the Keras Word2Vec model...\n",
            "Epoch 1/20\n",
            "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 9ms/step - accuracy: 0.5297 - loss: 0.6860 - val_accuracy: 0.4104 - val_loss: 0.7018\n",
            "Epoch 2/20\n",
            "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 7ms/step - accuracy: 0.6028 - loss: 0.6294 - val_accuracy: 0.4324 - val_loss: 0.7292\n",
            "Epoch 3/20\n",
            "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 9ms/step - accuracy: 0.6388 - loss: 0.6079 - val_accuracy: 0.4421 - val_loss: 0.7436\n",
            "Epoch 4/20\n",
            "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - accuracy: 0.6594 - loss: 0.5954 - val_accuracy: 0.4482 - val_loss: 0.7524\n",
            "Epoch 5/20\n",
            "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 8ms/step - accuracy: 0.6739 - loss: 0.5871 - val_accuracy: 0.4532 - val_loss: 0.7595\n",
            "Epoch 6/20\n",
            "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - accuracy: 0.6870 - loss: 0.5782 - val_accuracy: 0.4576 - val_loss: 0.7657\n",
            "Epoch 7/20\n",
            "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 9ms/step - accuracy: 0.6976 - loss: 0.5708 - val_accuracy: 0.4621 - val_loss: 0.7722\n",
            "Epoch 8/20\n",
            "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 8ms/step - accuracy: 0.7082 - loss: 0.5631 - val_accuracy: 0.4661 - val_loss: 0.7790\n",
            "Epoch 9/20\n",
            "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 8ms/step - accuracy: 0.7184 - loss: 0.5549 - val_accuracy: 0.4698 - val_loss: 0.7867\n",
            "Epoch 10/20\n",
            "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - accuracy: 0.7275 - loss: 0.5472 - val_accuracy: 0.4728 - val_loss: 0.7948\n",
            "Epoch 11/20\n",
            "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 8ms/step - accuracy: 0.7349 - loss: 0.5395 - val_accuracy: 0.4765 - val_loss: 0.8037\n",
            "Epoch 12/20\n",
            "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - accuracy: 0.7431 - loss: 0.5327 - val_accuracy: 0.4787 - val_loss: 0.8137\n",
            "Epoch 13/20\n",
            "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 9ms/step - accuracy: 0.7490 - loss: 0.5254 - val_accuracy: 0.4813 - val_loss: 0.8243\n",
            "Epoch 14/20\n",
            "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - accuracy: 0.7555 - loss: 0.5179 - val_accuracy: 0.4835 - val_loss: 0.8355\n",
            "Epoch 15/20\n",
            "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 9ms/step - accuracy: 0.7605 - loss: 0.5115 - val_accuracy: 0.4856 - val_loss: 0.8473\n",
            "Epoch 16/20\n",
            "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - accuracy: 0.7647 - loss: 0.5056 - val_accuracy: 0.4867 - val_loss: 0.8599\n",
            "Epoch 17/20\n",
            "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 9ms/step - accuracy: 0.7682 - loss: 0.5005 - val_accuracy: 0.4884 - val_loss: 0.8729\n",
            "Epoch 18/20\n",
            "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - accuracy: 0.7717 - loss: 0.4951 - val_accuracy: 0.4902 - val_loss: 0.8861\n",
            "Epoch 19/20\n",
            "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 9ms/step - accuracy: 0.7750 - loss: 0.4908 - val_accuracy: 0.4919 - val_loss: 0.8995\n",
            "Epoch 20/20\n",
            "\u001b[1m788/788\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - accuracy: 0.7776 - loss: 0.4865 - val_accuracy: 0.4930 - val_loss: 0.9133\n",
            "Training completed!\n"
          ]
        }
      ],
      "source": [
        "# Train the model\n",
        "print(\"Training the Keras Word2Vec model...\")\n",
        "### TODO ###\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
        "\n",
        "lr_scheduler = ReduceLROnPlateau(\n",
        "    monitor='val_accuracy',\n",
        "    factor=0.5,\n",
        "    patience=2,\n",
        "    min_lr=1e-6\n",
        ")\n",
        "history = model.fit(\n",
        "    x=[targets_np, contexts_np],\n",
        "    y=labels_all,\n",
        "    batch_size=2048,\n",
        "    epochs=20,\n",
        "    validation_split=0.1\n",
        ")\n",
        "\n",
        "### END OF TODO ###\n",
        "print(\"Training completed!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3efb200e",
      "metadata": {
        "id": "3efb200e"
      },
      "source": [
        "## Part 5. Extract embeddings and build a recommendation function"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5008a1bb",
      "metadata": {
        "id": "5008a1bb"
      },
      "source": [
        "After training, the most valuable part of our model is the **embedding table**.  \n",
        "- Each row in this table is a learned vector for one song.  \n",
        "- Songs that appear in similar playlist contexts should have embeddings that are close together in this vector space.  \n",
        "\n",
        "We can now build a simple recommendation function:  \n",
        "1. Given a song, look up its embedding.  \n",
        "2. Compare it to the embeddings of all other songs using **cosine similarity**.  \n",
        "3. Rank the results by similarity and return the top matches.  \n",
        "\n",
        "Cosine similarity measures how aligned two vectors are:  \n",
        "- Value near `1` $\\rightarrow$ songs are very similar.  \n",
        "- Value near `0` $\\rightarrow$ songs are unrelated.  \n",
        "\n",
        "**Your Task:** Complete and run the function below so that, when given a song name, it finds the most similar songs in the embedding space. Be sure to **exclude the song itself** from the results. This function will serve as our first recommender: a way to see whether the embeddings are capturing meaningful relationships."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "27076cd6",
      "metadata": {
        "id": "27076cd6"
      },
      "outputs": [],
      "source": [
        "# Extract embeddings and create recommendation function\n",
        "\n",
        "def find_similar_songs_keras(song_name, top_n=10):\n",
        "    \"\"\"\n",
        "    Find the top-N most similar songs to a given song, using cosine similarity\n",
        "    over the embeddings learned by the Keras Word2Vec model.\n",
        "\n",
        "    Args:\n",
        "        song_name: string, song in \"Title by Artist\" format.\n",
        "        top_n: how many similar songs to return.\n",
        "\n",
        "    Returns:\n",
        "        List of (song_name, similarity) tuples, ranked by similarity.\n",
        "        If the input song is not in the vocabulary, return an error string.\n",
        "    \"\"\"\n",
        "    if song_name not in song_to_id:\n",
        "        return f\"Song {song_name} not found in vocabulary\"\n",
        "\n",
        "    # Extract the learned embedding matrix from the model\n",
        "    embeddings = embedding_layer.get_weights()[0]\n",
        "    ### TODO ###\n",
        "    # Look up the embedding vector for the target song\n",
        "    target_idx = song_to_id[song_name]\n",
        "    target_embedding = embeddings[target_idx]\n",
        "\n",
        "    # Compute cosine similarity between target and all other songs\n",
        "    similarities = np.dot(embeddings, target_embedding) / (np.linalg.norm(embeddings, axis=1) * np.linalg.norm(target_embedding) + 1e-10\n",
        "    )\n",
        "    similarities[target_idx] = -1\n",
        "    # Get indices of top-N most similar songs\n",
        "    similar_indices = similarities.argsort()[-top_n:][::-1]\n",
        "    ### END OF TODO ###\n",
        "\n",
        "    # Convert back to song IDs and get similarities\n",
        "    similar_songs = []\n",
        "    for idx in similar_indices:\n",
        "        similar_song_name = id_to_song[idx]\n",
        "        similarity = similarities[idx]\n",
        "        similar_songs.append((similar_song_name, similarity))\n",
        "    return similar_songs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6a3b7e3c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6a3b7e3c",
        "outputId": "c02f0aa2-e14c-411f-d227-21c39bdfed62"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Keras Word2Vec recommendations for song Fade To Black by Metallica:\n",
            "  Don't Cry by Guns N' Roses: (similarity: 0.630)\n",
            "  Them Bones by Alice In Chains: (similarity: 0.585)\n",
            "  I Stand Alone by Godsmack: (similarity: 0.566)\n",
            "  Aenema by Tool: (similarity: 0.566)\n",
            "  No One Like You by Scorpions: (similarity: 0.559)\n",
            "  Prayin' For Daylight by Rascal Flatts: (similarity: 0.559)\n",
            "  All My Friends Say by Luke Bryan: (similarity: 0.559)\n",
            "  Beer!!! by Psychostick: (similarity: 0.558)\n",
            "  Juke Box Hero by Foreigner: (similarity: 0.556)\n",
            "  Fell On Black Days by Soundgarden: (similarity: 0.544)\n"
          ]
        }
      ],
      "source": [
        "# Test the recommendation function\n",
        "song_id = 2172  # Example: Fade To Black by Metallica\n",
        "test_song = songs_df.loc[song_id].title + ' by ' + songs_df.loc[song_id].artist # Using the same song as before\n",
        "print(f\"\\nKeras Word2Vec recommendations for song {test_song}:\")\n",
        "keras_recommendations = find_similar_songs_keras(test_song)\n",
        "\n",
        "if isinstance(keras_recommendations, str):\n",
        "    # If an error message is returned, print it directly\n",
        "    print(keras_recommendations)\n",
        "else:\n",
        "    for song, similarity in keras_recommendations:\n",
        "        print(f\"  {song}: (similarity: {similarity:.3f})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d962d889",
      "metadata": {
        "id": "d962d889"
      },
      "source": [
        "## Question 5.  \n",
        "Why do we bother training embeddings at all? Couldn't we simply build a co-occurrence matrix where each entry counts how often two songs appear together in playlists? If not, why not? If yes, what are the strengths and limitations of this baseline compared to learned embeddings, and in what cases might the co-occurrence approach be sufficient?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "046ef2b0",
      "metadata": {
        "id": "046ef2b0"
      },
      "source": [
        "## Question 6.\n",
        "We are building a recommender from playlists where no one explicitly rated or labeled the songs. Does that mean this approach is an unsupervised learning method, or should it be classified differently? Explain your reasoning by considering how the training pairs are created and whether labels are really absent or just implicit. Also, since we do not have explicit ratings, how can we evaluate whether the recommender is working well?"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "byVGkAJWXwNW"
      },
      "id": "byVGkAJWXwNW",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}